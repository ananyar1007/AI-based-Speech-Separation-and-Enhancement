{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2733eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\raghu\\\\test\\\\DNSSgit\\\\DNN-based_source_separation\\\\src')\n",
    "\n",
    "\n",
    "from utils.filterbank import choose_filterbank\n",
    "from utils.streaming_filterbank import choose_filterbank as choose_streaming_filterbank\n",
    "from modules.norm import CumulativeLayerNorm1d \n",
    "from modules.streaming_norm import CumulativeLayerNorm1d as StreamingCumulativeLayerNorm1d \n",
    "\n",
    "# now we can import the module in the parent\n",
    "# directory.\n",
    "#from src.models.streaming_encoder import Encoder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47548a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating streaming encoder\n",
      "-64\n",
      "Encoder(\n",
      "  (conv1d): StreamingConv1d(1, 512, kernel_size=(128,), stride=(64,), bias=False)\n",
      "  (nonlinear1d): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_basis=512\n",
    "kernel_size=128       \n",
    "chunk_size=kernel_size #Needs to be a multiple of kernel size\n",
    "stride=64\n",
    "enc_basis='trainable'\n",
    "dec_basis='trainable'\n",
    "n_sources=2\n",
    "encoder, decoder = choose_filterbank(n_basis, kernel_size=kernel_size, stride=stride, enc_basis=enc_basis, \n",
    "                                     dec_basis=dec_basis, enc_nonlinear='relu')\n",
    "streaming_encoder, streaming_decoder = choose_streaming_filterbank(n_basis, kernel_size=kernel_size, stride=stride, enc_basis=enc_basis, \n",
    "                                     dec_basis=dec_basis, enc_nonlinear='relu')\n",
    "norm1d = CumulativeLayerNorm1d(n_basis)\n",
    "streaming_norm1d = StreamingCumulativeLayerNorm1d(n_basis)\n",
    "\n",
    "# Now make that weights and bias for streaming_encoder and decoder are identical to the non streaming one.\n",
    "streaming_encoder.load_state_dict(encoder.state_dict())\n",
    "streaming_decoder.load_state_dict(decoder.state_dict())\n",
    "streaming_norm1d.load_state_dict(norm1d.state_dict())\n",
    "print(streaming_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48a2b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-mask w torch.Size([1, 1, 512, 29])\n",
      "Decoder input shape torch.Size([2, 512, 29])\n",
      "number of chunks 16\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 1])\n",
      "First output torch.Size([2, 1, 128])\n",
      "Decoder output torch.Size([2, 1, 64])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Streaming Conv: input size torch.Size([1, 1, 128])\n",
      "Decoder:Input before stat padding torch.Size([2, 512, 2])\n",
      "Decoder output torch.Size([2, 1, 128])\n",
      "Streaming Conv: State size torch.Size([1, 1, 64])\n",
      "Ref decoder output size torch.Size([2, 1, 1920])\n",
      "tensor(1.2032e-05, grad_fn=<NormBackward1>)\n",
      "torch.Size([2, 1, 1856])\n",
      "Encoder output tensor(1.6753e-05, grad_fn=<NormBackward1>)\n",
      "reshaped decoder output torch.Size([1, 2, 128])\n",
      "Final output size torch.Size([1, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "        \"\"\"\n",
    "        Args:\n",
    "            input (batch_size, C_in, T)\n",
    "        Returns:\n",
    "            output (batch_size, n_sources, T) or (batch_size, n_sources, C_in, T)\n",
    "            latent (batch_size, n_sources, n_basis, T'), where T' = (T-K)//S+1\n",
    "        \"\"\"\n",
    "        batch_size=1\n",
    "        \n",
    "        input = torch.randn(batch_size,1,1920)\n",
    "\n",
    "        n_dims = input.dim()\n",
    "\n",
    "        if n_dims == 3:\n",
    "            batch_size, C_in, T = input.size()\n",
    "            assert C_in == 1, \"input.size() is expected (?, 1, ?), but given {}\".format(input.size())\n",
    "        elif n_dims == 4:\n",
    "            batch_size, C_in, n_mics, T = input.size()\n",
    "            assert C_in == 1, \"input.size() is expected (?, 1, ?, ?), but given {}\".format(input.size())\n",
    "            input = input.view(batch_size, n_mics, T)\n",
    "        else:\n",
    "            raise ValueError(\"Not support {} dimension input\".format(n_dims))\n",
    "\n",
    "        padding = (stride - (T - kernel_size) % stride) % stride\n",
    "        padding_left = padding // 2\n",
    "        padding_right = padding - padding_left\n",
    "\n",
    "        input = F.pad(input, (padding_left, padding_right))\n",
    "        w = encoder(input)\n",
    "       # w = norm1d(w)\n",
    "        w_ref =  w.detach().clone()\n",
    "        w = w.unsqueeze(dim=1)\n",
    "        mask= torch.ones(batch_size,n_sources,w.size()[2], w.size()[3])\n",
    "    \n",
    "        print('pre-mask w',w.size())\n",
    "        w=w*mask\n",
    "        w = w.view(batch_size*n_sources, n_basis, -1)\n",
    "        print('Decoder input shape', w.size())\n",
    "        x = decoder(w)\n",
    "        # Split input into two parts and check if things work\n",
    "        w_total=torch.Tensor([])\n",
    "        x_total=torch.Tensor([])\n",
    "        num_chunks = input.size()[2]//chunk_size+1\n",
    "        print('number of chunks', num_chunks)\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i*chunk_size\n",
    "            end_idx = min((i+1)*chunk_size, input.size()[2])\n",
    "            input_slice = input[:,:,start_idx:end_idx]\n",
    "            w_s = streaming_encoder(input_slice)\n",
    "           # print('w_s.size',w_s.size())\n",
    "            if w_s is not None:\n",
    "             #   w_s = streaming_norm1d(w_s)\n",
    "                w_total=torch.cat([w_total,w_s], dim=2)\n",
    "                w_s = w_s.unsqueeze(dim=1)\n",
    "                streaming_mask = torch.ones(batch_size,n_sources,w_s.size()[2], w_s.size()[3])\n",
    "    \n",
    "                w_s=w_s*streaming_mask\n",
    "                w_s = w_s.view(batch_size*n_sources, n_basis, -1)\n",
    "                x_hat = streaming_decoder(w_s)\n",
    "                x_total=torch.cat([x_total,x_hat], dim=2)\n",
    "   #     print(w_ref.size(), w_total.size())\n",
    "   #     print(w_ref)\n",
    "   #     print(w_total)\n",
    "        print('Ref decoder output size',x.size()) \n",
    "        print(x_total.size())\n",
    "        # print(x)\n",
    "       # print(x_total)\n",
    "        print(torch.norm(x[:,:,:x_total.size()[2]]-x_total))\n",
    "      #  print(w[0,:,0])\n",
    "\n",
    "       # print(w_total[0,:,0])\n",
    "        print('Encoder output', torch.norm(w-w_total))\n",
    "  \n",
    "        if n_dims == 3:\n",
    "            x_hat = x_hat.view(batch_size, n_sources, -1)\n",
    "        else: # n_dims == 4\n",
    "            x_hat = x_hat.view(batch_size, n_sources, 1, -1)\n",
    "        print('reshaped decoder output', x_hat.size())\n",
    "        output = F.pad(x_hat, (-padding_left, -padding_right))\n",
    "        print('Final output size', output.size())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b560ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 8\n",
      "Parameter containing:\n",
      "tensor([[[ 0.0837,  0.1937,  0.1692, -0.1434]],\n",
      "\n",
      "        [[-0.4218, -0.1416,  0.4376, -0.3596]]], requires_grad=True)\n",
      "tensor([[[ 0.7244,  0.1181,  1.8424, -1.3256, -0.5561,  0.9746,  0.8712,\n",
      "          -0.4667, -1.1989,  1.0680,  0.6017,  0.4766],\n",
      "         [ 0.9860,  1.3744,  1.7378, -1.1001, -1.4678, -0.0912,  0.6194,\n",
      "          -0.5583, -0.8538, -1.4674,  0.7687,  0.7143]]])\n",
      "tensor([[[-3.5526e-01,  6.9052e-04, -1.5800e-02, -6.3019e-01,  4.2620e-02,\n",
      "          -4.0037e-01,  1.4253e+00, -9.9015e-01, -1.3312e-01,  6.8589e-01,\n",
      "          -6.1642e-01,  8.0929e-01, -6.3340e-02, -2.5957e-02,  6.1489e-01,\n",
      "          -3.5905e-01, -6.3487e-02,  1.5637e-01,  1.3186e-01,  8.9365e-01,\n",
      "          -7.3532e-01,  3.8215e-01,  1.7681e-01, -3.7154e-01,  3.9324e-01,\n",
      "          -3.2522e-01]]], grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([1, 1, 26])\n",
      "tensor([[[-3.5526e-01,  6.9052e-04, -1.5800e-02, -6.3019e-01,  4.2620e-02,\n",
      "          -4.0037e-01,  1.4253e+00, -9.9015e-01, -1.3312e-01,  6.8589e-01,\n",
      "          -7.3645e-01,  6.0759e-01]]], grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[ 0.5726,  0.1001, -0.6164,  0.8093, -0.0633, -0.0260,  0.6149,\n",
      "          -0.3590, -0.0635,  0.1564,  0.1319,  0.8937, -0.7353,  0.3822,\n",
      "           0.1768, -0.3715,  0.3932, -0.3252]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[-3.5526e-01,  6.9052e-04, -1.5800e-02, -6.3019e-01,  4.2620e-02,\n",
      "          -4.0037e-01,  1.4253e+00, -9.9015e-01, -1.3312e-01,  6.8589e-01,\n",
      "          -6.1642e-01,  8.0929e-01, -6.3340e-02, -2.5957e-02,  6.1489e-01,\n",
      "          -3.5905e-01, -6.3487e-02,  1.5637e-01,  1.3186e-01,  8.9365e-01,\n",
      "          -7.3532e-01,  3.8215e-01,  1.7681e-01, -3.7154e-01,  3.9324e-01,\n",
      "          -3.2522e-01]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,2,12)\n",
    "stride=2\n",
    "print(decoder.kernel_size, decoder.stride)\n",
    "myMod = torch.nn.ConvTranspose1d(2,1,kernel_size=4,stride=2, bias=False)\n",
    "print(myMod.weight)\n",
    "z = myMod(x)\n",
    "print(x)\n",
    "print(z)\n",
    "print(z.size())\n",
    "\n",
    "x1 = x[:,:,0:5]\n",
    "z1 = myMod(x1)\n",
    "print(z1)\n",
    "x2 = x[:,:,4:12]\n",
    "z2=myMod(x2)\n",
    "print(z2)\n",
    "z_out = torch.cat([z1[:,:,0:-stride], z2[:,:,stride:]], dim=2)\n",
    "print(z_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=myMod.weight\n",
    "print(wd.size())\n",
    "# Getting the outputs\n",
    "print(wd[0,0,0]*x[0,0,0]+wd[1,0,0]*x[0,1,0])\n",
    "print(wd[0,0,1]*x[0,0,0]+wd[1,0,1]*x[0,1,0] )\n",
    "print(wd[0,0,2]*x[0,0,0]+wd[1,0,2]*x[0,1,0]+wd[0,0,0]*x[0,0,1]+wd[1,0,0]*x[0,1,1] )\n",
    "print(wd[0,0,3]*x[0,0,0]+wd[1,0,3]*x[0,1,0]+wd[0,0,1]*x[0,0,1]+wd[1,0,1]*x[0,1,1] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from models.tdcn import TimeDilatedConvNet\n",
    "    batch_size = 1\n",
    "    T = 40\n",
    "    in_channels, out_channels, skip_channels = 2, 2, 2\n",
    "    kernel_size = 3\n",
    "    num_blocks = 1\n",
    "    num_layers = 4\n",
    "    dilated, separable = True, True\n",
    "    causal = True\n",
    "    nonlinear = 'prelu'\n",
    "    norm = False\n",
    "\n",
    "    input = torch.randn((batch_size, in_channels, T), dtype=torch.float)\n",
    "\n",
    "    model = TimeDilatedConvNet(in_channels, hidden_channels=out_channels, skip_channels=skip_channels, kernel_size=kernel_size, num_blocks=num_blocks, num_layers=num_layers, dilated=dilated, separable=separable, causal=causal, nonlinear=nonlinear, norm=norm)\n",
    "\n",
    "    output = model(input)\n",
    "\n",
    "    print(input.size())\n",
    "   # print(input)\n",
    "    print(output.size())\n",
    "    print(output)\n",
    "    \n",
    "# MAgic numbers for num_layer=2 (2+4=6)\n",
    "# For num_layers=3 (2+4+8 =14)\n",
    "# For num_layers=4 (2+4+8+16 =26)\n",
    "    input1 = input[:,:,:30]\n",
    "    input2 = input[:,:,4:]\n",
    "    \n",
    "    output1 = model(input1)\n",
    "    output2 = model(input2)\n",
    "    print(output1)\n",
    "    print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3661879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class StreamingConv1d(nn.Conv1d):\n",
    "    def __init__(self,in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\n",
    "        \n",
    "        self.state=None\n",
    "        self.state_len = self.kernel_size[0]*self.dilation[0]-self.stride[0]*self.dilation[0]\n",
    "        print(-self.kernel_size[0]*self.dilation[0]+self.stride[0])\n",
    "    def forward(self, input):\n",
    "        # input is (1,kL)\n",
    "        # output is (1,k,N)\n",
    "      #  print('Encoder:State',self.state.size())\n",
    "      #  print('Encoder:Input before stat padding',input.size())\n",
    "        x=None\n",
    " #       print('input size',input.size())\n",
    "        if self.state is None:\n",
    "            sz_size=0\n",
    "        else:\n",
    "            sz_size=self.state.size()[2]\n",
    "            print('Streaming Conv: State size',self.state.size())\n",
    "            \n",
    "        if sz_size+input.size()[2] >(self.kernel_size[0]-1)*self.dilation[0]:\n",
    "            print('Streaming Conv: input size',input.size())\n",
    "            if sz_size > 0:\n",
    "                extended_input = torch.cat([self.state, input], dim=2)\n",
    "            else:\n",
    "                extended_input = input\n",
    "            x = super().forward(extended_input)\n",
    "         #  print('Encoder:state+input dim', input.size()) \n",
    "        # What to do if the input is too short?\n",
    "        # Add it to state, return empty!\n",
    "        \n",
    "        if sz_size > 0:\n",
    "            self.state = torch.cat([self.state, input[:,:,-(self.kernel_size[0]-1)*self.dilation[0]:]], dim=2)\n",
    "        else:\n",
    "            self.state =  input[:,:,-(self.kernel_size[0]-1)*self.dilation[0]:]\n",
    "            \n",
    "# Keep the needed elements\n",
    " #       print(\"State size, before pruning\", self.state.size())\n",
    "        self.state = self.state[:,:,-(self.kernel_size[0]-1)*self.dilation[0]:]\n",
    " #       print(\"State size, after pruning\", self.state.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57368eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "out_channels=2\n",
    "kernel_size=3\n",
    "stride=2\n",
    "padding=0\n",
    "dilation=4\n",
    "bias=False\n",
    "\n",
    "input = torch.randn(batch_size,1,30)\n",
    "\n",
    "m1 = nn.Conv1d(1,out_channels, kernel_size,stride,padding,dilation,1,bias)\n",
    "m2 = StreamingConv1d(1,out_channels, kernel_size,stride,padding,dilation,1,bias)\n",
    "m2.weight=m1.weight\n",
    "\n",
    "ref = m1(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87405b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=8\n",
    "out_total=torch.Tensor([])\n",
    "num_chunks = input.size()[2]//chunk_size+1\n",
    "print('number of chunks', num_chunks)\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i*chunk_size\n",
    "    end_idx = min((i+1)*chunk_size, input.size()[2])\n",
    "    input_slice = input[:,:,start_idx:end_idx]\n",
    "    out_slice =m2(input_slice)\n",
    "#    print('Input slice', input_slice.size())\n",
    "    if out_slice is not None:\n",
    "        print('Out slice', out_slice.size())\n",
    "        out_total=torch.cat([out_total,out_slice], dim=2)\n",
    "        print('Out_total', out_total.size())\n",
    "print(ref.size(), out_total.size())\n",
    "print(ref)\n",
    "print(out_total)\n",
    "print(torch.norm(out_total-ref))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa153f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tasnet import choose_layer_norm\n",
    "class StreamingDepthwiseSeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=256, skip_channels=256, kernel_size=3, stride=2, dilation=1, causal=True, nonlinear=None, norm=True,use_batch_norm=False, dual_head=True, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.dual_head = dual_head\n",
    "        self.norm = norm\n",
    "        self.eps = eps\n",
    "\n",
    "        self.depthwise_conv1d = StreamingConv1d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, groups=in_channels)\n",
    "\n",
    "        if nonlinear is not None:\n",
    "            if nonlinear == 'prelu':\n",
    "                self.nonlinear1d = nn.PReLU()\n",
    "            else:\n",
    "                raise ValueError(\"Not support {}\".format(nonlinear))\n",
    "            self.nonlinear = True\n",
    "        else:\n",
    "            self.nonlinear = False\n",
    "\n",
    "        if norm:\n",
    "             if use_batch_norm:\n",
    "                 norm_name='BN'\n",
    "             else:\n",
    "                 norm_name = 'cLN' if causal else 'gLN'\n",
    "             self.norm1d = StreamingCumulativeLayerNorm1d(in_channels)\n",
    "        if dual_head:\n",
    "            self.output_pointwise_conv1d = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "        self.skip_pointwise_conv1d = nn.Conv1d(in_channels, skip_channels, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        nonlinear, norm = self.nonlinear, self.norm\n",
    "        dual_head = self.dual_head\n",
    "\n",
    "        x = self.depthwise_conv1d(input)\n",
    "\n",
    "        if nonlinear:\n",
    "            x = self.nonlinear1d(x)\n",
    "        if norm:\n",
    "            if x is not None:\n",
    "                print('before norm', x.size())\n",
    "                x = self.norm1d(x)\n",
    "\n",
    "        if x is None:\n",
    "            return None,None\n",
    "\n",
    "        if dual_head:\n",
    "            output = self.output_pointwise_conv1d(x)\n",
    "        else:\n",
    "            output = None\n",
    "\n",
    "        skip = self.skip_pointwise_conv1d(x)\n",
    "\n",
    "        return output,skip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c420deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=256, skip_channels=256, kernel_size=3, stride=2, dilation=1, causal=True, nonlinear=None, norm=True,use_batch_norm=False, dual_head=True, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dual_head = dual_head\n",
    "        self.norm = norm\n",
    "        self.eps = eps\n",
    "\n",
    "        self.depthwise_conv1d = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, groups=in_channels)\n",
    "\n",
    "        if nonlinear is not None:\n",
    "            if nonlinear == 'prelu':\n",
    "                self.nonlinear1d = nn.PReLU()\n",
    "            else:\n",
    "                raise ValueError(\"Not support {}\".format(nonlinear))\n",
    "            self.nonlinear = True\n",
    "        else:\n",
    "            self.nonlinear = False\n",
    "\n",
    "        if norm:\n",
    "             if use_batch_norm:\n",
    "                 norm_name='BN'\n",
    "             else:\n",
    "                 norm_name = 'cLN' if causal else 'gLN'\n",
    "             self.norm1d = CumulativeLayerNorm1d(in_channels)\n",
    "        if dual_head:\n",
    "            self.output_pointwise_conv1d = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "        self.skip_pointwise_conv1d = nn.Conv1d(in_channels, skip_channels, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        nonlinear, norm = self.nonlinear, self.norm\n",
    "        dual_head = self.dual_head\n",
    "\n",
    "        x = self.depthwise_conv1d(input)\n",
    "\n",
    "        if nonlinear:\n",
    "            x = self.nonlinear1d(x)\n",
    "        if norm:\n",
    "            x = self.norm1d(x)\n",
    "\n",
    "    \n",
    "\n",
    "        if dual_head:\n",
    "            output = self.output_pointwise_conv1d(x)\n",
    "        else:\n",
    "            output = None\n",
    "\n",
    "        skip = self.skip_pointwise_conv1d(x)\n",
    "\n",
    "        return output,skip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6aac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models.tdcn import DepthwiseSeparableConv1d\n",
    "\n",
    "ref_mod = DepthwiseSeparableConv1d(2,4,4,3,stride=1,dilation=64,causal=True, norm=True)\n",
    "streaming_mod = StreamingDepthwiseSeparableConv1d(2,4,4,3,stride=1,dilation=64,causal=True, norm=True)\n",
    "\n",
    "streaming_mod.load_state_dict(ref_mod.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899af0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = torch.randn(1,2,200)\n",
    "y,skip=ref_mod(x)\n",
    "chunk_size=8\n",
    "out_total=torch.Tensor([])\n",
    "skip_total=torch.Tensor([])\n",
    "num_chunks = np.ceil(x.size()[2]/chunk_size).astype(np.int32)\n",
    "print('number of chunks', num_chunks)\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i*chunk_size\n",
    "    end_idx = min((i+1)*chunk_size, x.size()[2])\n",
    "    input_slice = x[:,:,start_idx:end_idx]\n",
    "    print('Chunk Input slice', input_slice.size())\n",
    "    \n",
    "    out_slice, skip_slice =streaming_mod(input_slice)\n",
    "    if out_slice is not None:\n",
    "        print('Out slice', out_slice.size())\n",
    "        out_total=torch.cat([out_total,out_slice], dim=2)\n",
    "        print('Out_total', out_total.size())\n",
    "    if skip_slice is not None:\n",
    "        print('skip slice', skip_slice.size())\n",
    "        skip_total=torch.cat([skip_total,skip_slice], dim=2)\n",
    "        print('skip_total', skip_total.size())\n",
    "print(torch.norm(y-out_total))\n",
    "print(torch.norm(skip-skip_total))\n",
    "print(y)\n",
    "print(y.size())\n",
    "print(out_total.size())\n",
    "print(out_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003cab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingResidualBlock1d(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels=256, skip_channels=256, kernel_size=3, stride=2, dilation=1,\n",
    "                 separable=False, causal=True, nonlinear=None, norm=True, use_batch_norm=False, dual_head=True, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size, self.stride, self.dilation = kernel_size, stride, dilation\n",
    "        self.separable, self.causal = separable, causal\n",
    "        self.norm = norm\n",
    "        self.dual_head = dual_head\n",
    "\n",
    "        self.bottleneck_conv1d = nn.Conv1d(num_features, hidden_channels, kernel_size=1, stride=1)\n",
    "\n",
    "        if nonlinear is not None:\n",
    "            if nonlinear == 'prelu':\n",
    "                self.nonlinear1d = nn.PReLU()\n",
    "            else:\n",
    "                raise ValueError(\"Not support {}\".format(nonlinear))\n",
    "            self.nonlinear = True\n",
    "        else:\n",
    "            self.nonlinear = False\n",
    "\n",
    "        if norm:\n",
    "            if use_batch_norm:\n",
    "                norm_name='BN'\n",
    "            else:\n",
    "                norm_name = 'cLN' if causal else 'gLN'\n",
    "            self.norm1d = StreamingCumulativeLayerNorm1d(hidden_channels)\n",
    "            #choose_layer_norm(norm_name, hidden_channels, causal=causal, eps=eps)\n",
    "        if separable:\n",
    "            self.separable_conv1d = StreamingDepthwiseSeparableConv1d(hidden_channels, num_features, skip_channels=skip_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, causal=causal, nonlinear=nonlinear, norm=norm, use_batch_norm=use_batch_norm, dual_head=dual_head, eps=eps)\n",
    "        else:\n",
    "            if dual_head:\n",
    "                self.output_conv1d = nn.Conv1d(hidden_channels, num_features, kernel_size=kernel_size, dilation=dilation)\n",
    "            self.skip_conv1d = nn.Conv1d(hidden_channels, skip_channels, kernel_size=kernel_size, dilation=dilation)\n",
    "        self.first_time=True\n",
    "\n",
    "    def forward(self, input):\n",
    "        kernel_size, stride, dilation = self.kernel_size, self.stride, self.dilation\n",
    "        nonlinear, norm = self.nonlinear, self.norm\n",
    "        separable, causal = self.separable, self.causal\n",
    "        dual_head = self.dual_head\n",
    "\n",
    "        _, _, T_original = input.size()\n",
    "\n",
    "        residual = input\n",
    "        x = self.bottleneck_conv1d(input)\n",
    "\n",
    "        if nonlinear:\n",
    "            x = self.nonlinear1d(x)\n",
    "        if norm:\n",
    "            if x is not None:\n",
    "                x = self.norm1d(x)\n",
    "        \n",
    "        print('T_original', T_original)\n",
    "        padding = (T_original - 1) * stride - T_original + (kernel_size - 1) * dilation + 1\n",
    "        print('padding', padding)\n",
    "        if self.first_time:\n",
    "            if causal:\n",
    "                padding_left = padding\n",
    "                padding_right = 0\n",
    "            else:\n",
    "                padding_left = padding//2\n",
    "                padding_right = padding - padding_left\n",
    "            self.first_time=False\n",
    "            x = F.pad(x, (padding_left, padding_right))\n",
    "\n",
    "        if separable:\n",
    "            output, skip = self.separable_conv1d(x) # output may be None\n",
    "        else:\n",
    "            if dual_head:\n",
    "                output = self.output_conv1d(x)\n",
    "            else:\n",
    "                output = None\n",
    "\n",
    "            skip = self.skip_conv1d(x)\n",
    "        if output is not None:\n",
    "            print(output.size(), residual.size())\n",
    "            output = output + residual\n",
    "            return output, skip\n",
    "        else:\n",
    "            return None,skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc84316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tdcn import ResidualBlock1d\n",
    "ref_mod=ResidualBlock1d(2,4,4,3,stride=1,dilation=4,separable=True,norm=True)\n",
    "streaming_mod=StreamingResidualBlock1d(2,4,4,3,stride=1,dilation=4,separable=True,norm=True)\n",
    "streaming_mod.load_state_dict(ref_mod.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,2,30)\n",
    "y_ref, skip_ref=ref_mod(x)\n",
    "chunk_size=20\n",
    "out_total=torch.Tensor([])\n",
    "skip_total =torch.Tensor([])\n",
    "num_chunks = np.ceil(x.size()[2]/chunk_size).astype(np.int32)\n",
    "print('number of chunks', num_chunks)\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i*chunk_size\n",
    "    end_idx = min((i+1)*chunk_size, x.size()[2])\n",
    "    input_slice = x[:,:,start_idx:end_idx]\n",
    "    print('Chunk Input slice', input_slice.size())\n",
    "    \n",
    "    out_slice, skip_slice =streaming_mod(input_slice)\n",
    "    if out_slice is not None:\n",
    "        print('Out slice', out_slice.size())\n",
    "        out_total=torch.cat([out_total,out_slice], dim=2)\n",
    "        print('Out_total', out_total.size())\n",
    "    if skip_slice is not None:\n",
    "        print('Skip slice', skip_slice.size())\n",
    "        skip_total=torch.cat([skip_total,skip_slice], dim=2)\n",
    "        print('Skip_total', skip_total.size())\n",
    "\n",
    "print(torch.norm(y_ref-out_total))                             \n",
    "print(y_ref.size())\n",
    "print(out_total.size())\n",
    "print(y_ref)\n",
    "print(out_total)\n",
    "      \n",
    "print(skip_ref.size())\n",
    "print(skip_total.size())\n",
    "print(skip_ref)\n",
    "print(skip_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.conv_tasnet import ConvTasNet\n",
    "ref = ConvTasNet(\n",
    "        n_basis=32, kernel_size=16, stride=None, enc_basis='trainable', dec_basis='trainable',\n",
    "        sep_hidden_channels=256, sep_bottleneck_channels=128, sep_skip_channels=128, sep_kernel_size=3, sep_num_blocks=3, sep_num_layers=8,\n",
    "        dilated=True, separable=True,\n",
    "        sep_nonlinear='prelu', sep_norm=True,use_batch_norm=False, mask_nonlinear='sigmoid',\n",
    "        causal=True,\n",
    "        n_sources=2,\n",
    "        eps=1e-5,\n",
    "        enc_nonlinear='relu'\n",
    "    )\n",
    "\n",
    "x= torch.randn(1,1,10000)\n",
    "ref(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e2950",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "class StreamingTimeDilatedConvNet(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels=256, skip_channels=256, kernel_size=3, num_blocks=3, num_layers=10, dilated=True, separable=False, causal=True,use_batch_norm=False, nonlinear=None, norm=True, eps=EPS):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        net = []\n",
    "\n",
    "        for idx in range(num_blocks):\n",
    "            if idx == num_blocks - 1:\n",
    "                net.append(StreamingTimeDilatedConvBlock1d(num_features, hidden_channels=hidden_channels, skip_channels=skip_channels, kernel_size=kernel_size, num_layers=num_layers, dilated=dilated, separable=separable, causal=causal, nonlinear=nonlinear, norm=norm, use_batch_norm=use_batch_norm, dual_head=False, eps=eps))\n",
    "            else:\n",
    "                net.append(StreamingTimeDilatedConvBlock1d(num_features, hidden_channels=hidden_channels, skip_channels=skip_channels, kernel_size=kernel_size, num_layers=num_layers, dilated=dilated, separable=separable, causal=causal, nonlinear=nonlinear, norm=norm, use_batch_norm=use_batch_norm, dual_head=True, eps=eps))\n",
    "\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, input):\n",
    "        num_blocks = self.num_blocks\n",
    "\n",
    "        x = input\n",
    "        skip_connection = 0\n",
    "\n",
    "        for idx in range(num_blocks):\n",
    "            x, skip = self.net[idx](x)\n",
    "            x_size=0\n",
    "            skip_size=0\n",
    "            if x is not None:\n",
    "                x_size=x.size()\n",
    "            if skip is not None:\n",
    "                skip_size=skip.size()\n",
    "            print('Streaming TDCN', idx, x_size,skip_size)\n",
    "            skip_connection = skip_connection + skip\n",
    "            print('Streaming TDCN 2', idx, skip_connection.size())\n",
    "\n",
    "        output = skip_connection\n",
    "\n",
    "        return output\n",
    "\n",
    "class StreamingTimeDilatedConvBlock1d(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels=256, skip_channels=256, kernel_size=3, num_layers=10, dilated=True, separable=False, causal=True, nonlinear=None, norm=True, use_batch_norm=False, dual_head=True, eps=EPS):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        net = []\n",
    "        print('tdcn: Dilated:', dilated)\n",
    "        for idx in range(num_layers):\n",
    "            if dilated:\n",
    "                dilation = 2**idx\n",
    "                print('Dilation', idx, dilation)\n",
    "                stride = 1\n",
    "            else:\n",
    "                dilation = 1\n",
    "                stride = 2\n",
    "            if not dual_head and idx == num_layers - 1:\n",
    "                net.append(StreamingResidualBlock1d(num_features, hidden_channels=hidden_channels, skip_channels=skip_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, separable=separable, causal=causal, nonlinear=nonlinear, norm=norm, use_batch_norm=use_batch_norm, dual_head=False, eps=eps))\n",
    "            else:\n",
    "                net.append(StreamingResidualBlock1d(num_features, hidden_channels=hidden_channels, skip_channels=skip_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, separable=separable, causal=causal, nonlinear=nonlinear, norm=norm, use_batch_norm=use_batch_norm, dual_head=True, eps=eps))\n",
    "\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, input):\n",
    "        num_layers = self.num_layers\n",
    "\n",
    "        x = input\n",
    "        skip_connection = 0\n",
    "\n",
    "        for idx in range(num_layers):\n",
    "            x, skip = self.net[idx](x)\n",
    "            skip_connection = skip_connection + skip\n",
    "\n",
    "        return x, skip_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_basis=32\n",
    "kernel_size=16\n",
    "stride=None\n",
    "enc_basis='trainable'\n",
    "dec_basis='trainable',\n",
    "sep_hidden_channels=256\n",
    "sep_bottleneck_channels=2\n",
    "sep_skip_channels=2\n",
    "sep_kernel_size=3\n",
    "sep_num_blocks=3\n",
    "sep_num_layers=8\n",
    "dilated=True\n",
    "separable=True\n",
    "sep_nonlinear='prelu'\n",
    "sep_norm=True\n",
    "use_batch_norm=False\n",
    "mask_nonlinear='sigmoid'\n",
    "causal=True\n",
    "n_sources=2\n",
    "eps=1e-5\n",
    "enc_nonlinear='relu'\n",
    "\n",
    "ref_mod=TimeDilatedConvNet(\n",
    "            num_features=sep_bottleneck_channels, hidden_channels=sep_hidden_channels, skip_channels=sep_skip_channels, \n",
    "            kernel_size=sep_kernel_size, num_blocks=sep_num_blocks, num_layers=sep_num_layers,\n",
    "            dilated=dilated, separable=separable, causal=causal, nonlinear=sep_nonlinear, norm=True\n",
    "        )\n",
    "streaming_mod =StreamingTimeDilatedConvNet(\n",
    "            num_features=sep_bottleneck_channels, hidden_channels=sep_hidden_channels, skip_channels=sep_skip_channels, \n",
    "            kernel_size=sep_kernel_size, num_blocks=sep_num_blocks, num_layers=sep_num_layers,\n",
    "            dilated=dilated, separable=separable, causal=causal, nonlinear=sep_nonlinear, norm=True\n",
    "        )\n",
    "\n",
    "streaming_mod.load_state_dict(ref_mod.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,2,40)\n",
    "y1=ref_mod(x)\n",
    "print('ref output',y1.size())\n",
    "chunk_size=20\n",
    "out_total=torch.Tensor([])\n",
    "num_chunks = np.ceil(x.size()[2]/chunk_size).astype(np.int32)\n",
    "print('number of chunks', num_chunks)\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i*chunk_size\n",
    "    end_idx = min((i+1)*chunk_size, x.size()[2])\n",
    "    input_slice = x[:,:,start_idx:end_idx]\n",
    "    print('Chunk Input slice', input_slice.size())\n",
    "    \n",
    "    out_slice =streaming_mod(input_slice)\n",
    "\n",
    "    if out_slice is not None:\n",
    "        print('Out slice', out_slice.size())\n",
    "        out_total=torch.cat([out_total,out_slice], dim=2)\n",
    "        print('Out_total', out_total.size())\n",
    "print(y1)\n",
    "print(y1.size())\n",
    "print(out_total.size())\n",
    "print(out_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f68a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34299550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
